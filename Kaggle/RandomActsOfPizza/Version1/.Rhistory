sum(width(disjoined[ !in.both ]))
z = GRanges("chr1", IRanges(c(101,201,401,501),c(150,250,450,550)), strand="-")
plotGRanges(z)
x%over%z
###################################Part 5#################################
library(ERBS)
data(HepG2)
data(GM12878)
HepG2[17, ]
start(HepG2[17, ])
distanceToNearest(HepG2[17, ], GM12878)
idx = distanceToNearest(HepG2[17, ], GM12878)
GM12878[idx, ]
idx$subjectHits
GM12878[945, ]
start(GM12878[945, ])
idx = subjectHIts(distances)
idx = subjectHits(distances)
distances = distanceToNearest(HepG2[17, ], GM12878)
idx = subjectHits(distances)
start(GM12878[idx, ])
distances
mcols(distances)$distance
distances = distanceToNearest(HepG2, GM12878)
distances
mcols(distances)$distance < 2000
sum(mcols(distances)$distance < 2000)
sum(mcols(distances)$distance < 2000) / 303
sum(mcols(distances)$distance < 2000) / length(distances)
library(Homo.sapiens)
ghs = genes(Homo.sapiens)
ghs
mcols(ghs)
table(seqnames(ghs))
max(table(seqnames(ghs)))
which.max(table(seqnames(ghs)))
hist(width(ghs))
par(mfrow = c(1,1))
hist(width(ghs))
class(ghs)
median(width(ghs))
res = findOverlaps(HepG2,GM12878)
erbs = HepG2[queryHits(res)]
erbs = granges(erbs)
erbs2= intersect(HepG2,GM12878)
erbs
erbs2
library(Homo.sapiens)
ghs = genes(Homo.sapiens)
tss <- resize(ghs,1)
tss[unlist(mcols(tss)$GENEID==100113402),]
start(tss['100113402'])
library(ERBS)
data(HepG2)
data(GM12878)
res = findOverlaps(HepG2,GM12878)
erbs = HepG2[queryHits(res)]
erbs = granges(erbs)
erbs[4, ]
distanceToNearest(erbs[4, ], tss)
distances = distanceToNearest(erbs[4, ], tss)
subjectHits(distances)
index = subjectHits(distances)
tss[index]
?select()
?select
key = as.character(values(tss[index]$GENEID))
key
key = as.character(values(tss[index])$GENEID)
key
select(Homo.sapiens, keys = key, keytype = 'GENEID', columns = c('SYMBOL'))
library(ERBS)
data(HepG2)
data(GM12878)
res = findOverlaps(HepG2,GM12878)
erbs = HepG2[queryHits(res)]
erbs = granges(erbs)
erbs
genome(erbs)
erbs
library(BSgenome.Hsapiens.UCSC.hg19)
reqSeq = getSeq(erbs)
reqSeq = getSeq(erbs)
library(biovizBase)
source("http://bioconductor.org/biocLite.R")
biocLite("BSgenome")
?alphabetFrequency
reqSeq = getSeq(erbs)
library(BSgenome)
reqSeq = getSeq(erbs)
erbs = granges(erbs)
erbs
#Q2.8.2
library(BSgenome.Hsapiens.UCSC.hg19)
source("http://bioconductor.org/biocLite.R")
library(BSgenome)
reqSeq = getSeq(erbs)
reqSeq = getSeq(HSapiens, erbs)
HSapiens
Hsapiens
reqSeq = getSeq(Hsapiens, erbs)
reqSeq = getSeq(Hsapiens, HepG2)
?getSeq
helpSeq = getSeq(Hsapiens, HepG2)
HelpG2
HepG2
helpSeq = getSeq(Hsapiens, HepG2)
alphabetFrequency(getSeq(Hsapiens,erbs), baseOnly=T, as.prob=T)
getSeq(Hsapiens, erbs)
biocLite("BSgenome.Hsapiens.UCSC.hg19")
library(BSgenome.Hsapiens.UCSC.hg19)
library(BSgenome)
alphabetFrequency(getSeq(Hsapiens,erbs), baseOnly=T, as.prob=T)
library(BSgenome.Hsapiens.UCSC.hg19)
biocLite("BSgenome")
library(BSgenome)
alphabetFrequency(getSeq(Hsapiens,erbs), baseOnly=T, as.prob=T)
erbs = granges(erbs)
library(ERBS)
data(HepG2)
data(GM12878)
res = findOverlaps(HepG2,GM12878)
erbs = HepG2[queryHits(res)]
erbs = granges(erbs)
alphabetFrequency(getSeq(Hsapiens,erbs), baseOnly=T, as.prob=T)
alphabetFrequency(getSeq(Hsapiens,erbs))
alphaFreq = alphabetFrequency(getSeq(Hsapiens,erbs))
str(alphaFreq)
gcContent = alphaFreq
gcContent$Prop = gcContent[, 'C'] + gcContent[, 'G']
gcContent
gcContent = alphaFreq
gcContent = as.data.frame(alphaFreq)
gcContent
gcContent$Prop = gcContent$G + gcContent$C
gcContent
gcContent = as.data.frame(alphaFreq)
gcContent
gcContent$Prop = (gcContent$G + gcContent$C) / (gcContent$A + gcContent$C + gcContent$G + gcContent$T)
gcContent
median(gcContent$Prop)
controlData = getSeq(Hsapiens, shift(erbs, 1000))
alphaFreq = alphabetFrequency(getSeq(Hsapiens,controlData))
controlData = getSeq(Hsapiens, shift(erbs, 10000))
controlData
erbs
alphaFreq = alphabetFrequency(controlData)
alphaFreq = alphabetFrequency(controlData)[, 2:3]
str(alphaFreq)
n = width(alphaFreq)
n = width(shift(erbs, 10000))
gccontent = rowSums(alphaFreq)/n
median(gccontent)
rm(list = ls())
library(Biostrings)
available.genome()
available.genomes()
grep('Drerio', available.genomes(), ignore.case = TRUE, values = TRUE)
grep('Drerio', available.genomes(), ignore.case = TRUE)
length(grep('Drerio', available.genomes(), ignore.case = TRUE))
?grep
grep('Drerio', available.genomes(), ignore.case = TRUE, value = TRUE)
library(BSgenome.Hsapiens.UCSC.hg19.masked)
biocLite('BSgenome.Hsapiens.UCSC.hg19.masked')
library(BSgenome.Hsapiens.UCSC.hg19.masked)
c17m = BSgenome.Hsapiens.UCSC.hg19.masked$chr17
class(c17m)
c22m = BSgenome.Hsapiens.UCSC.hg19.masked$chr22
length(c22m)
n = length(c22m)
names(c22m)
unique(c22m$desc)
mcols(c22m)
?mcols
masks(c22m)
masks(c22m)$AGAPS
width(masks(c22m)$AGAPS)
sum (width(masks(c22m)$AGAPS))
den = length(c22m)
num = sum (width(masks(c22m)$AGAPS))
ans = 100 * num / den
ans
round(100*sum(width(masks(c22m)$AGAPS))/length(c22m),0)
ans = round(100 * num / den, 0)
ans
"hg19ToHg38.over.chain.gz")
download.file("http://hgdownload.cse.ucsc.edu/goldenPath/hg19/liftOver/hg19ToHg38.over.chain.gz",
"hg19ToHg38.over.chain.gz")
library(R.utils)
gunzip("hg19ToHg38.over.chain.gz")
library(ERBS)
data(HepG2)
library(rtracklayer)
ch = import.chain("hg19ToHg38.over.chain")
nHepG2 = liftOver(HepG2, ch)
hHepG2
nHepG2
start(nHepG2)
start(nHepG2)[[1]]
start(nHepG2)[[1]] - start(HepG2)[[1]]
library(devtools)
install_github("genomicsclass/ph525x")
library(ph525x)
stopifnot(packageVersion("ph525x") >= "0.0.16") # do over if fail
modPlot("ESR1", useGeneSym=FALSE, collapse=FALSE)
ESR1
ESR1
source("http://bioconductor.org/biocLite.R")
biocLite("Gviz")
modPlot("ESR1", useGeneSym=FALSE, collapse=FALSE)
library(Gviz)
biocLite("Gviz")
library(Gviz)
source("http://bioconductor.org/biocLite.R")
biocLite("Gviz")
library(Gviz)
install.packages('acepack')
library(acepack)
install.packages("acepack")
library(acepack)
source("http://bioconductor.org/biocLite.R")
biocLite("Gviz")
library(Gviz)
modPlot("ESR1", useGeneSym=FALSE, collapse=FALSE)
library(BiocInstaller)
biocLite("Gviz")
library(Gviz)
library(acepack)
library(Gviz)
install.packages('Hmisc')
library(Hmisc)
library(Gviz)
rm(list = ls())
rm(list = ls())
getwd()
setwd("/Users/admin/Documents/Coursera/AnalyticsEdge-edX/Kaggle/RandomActsOfPizza/Version1")
getwd()
#install.packages('rjson')
library(rjson)
#Step1: Convert json file to data frame. Found the code for this here:
#https://www.kaggle.com/benhamner/random-acts-of-pizza/rmarkdown-default-text
trainJSON = fromJSON(file = 'train.json')
testJSON = fromJSON(file = 'test.json')
# Helper functions for converting the JSON data to a dataframe
processCell <- function(cell) {
if (is.null(cell) || length(cell) == 0) {
return(NA)
}
if (length(cell)>1) {
return(paste(cell, collapse='; '))
}
return(cell)
}
processRow <- function(row) unlist(lapply(row, processCell))
train <- as.data.frame(do.call("rbind", lapply(trainJSON, processRow)))
str(train)
test = as.data.frame(do.call('rbind', lapply(testJSON, processRow)))
str(test)
#Data preprocessing
numNames = c("number_of_downvotes_of_request_at_retrieval", "number_of_upvotes_of_request_at_retrieval",
"request_number_of_comments_at_retrieval", "requester_account_age_in_days_at_request" ,
"requester_account_age_in_days_at_retrieval", "requester_days_since_first_post_on_raop_at_request",
"requester_days_since_first_post_on_raop_at_retrieval", "requester_number_of_comments_at_request" ,
"requester_number_of_comments_in_raop_at_retrieval", "requester_number_of_posts_at_request",
"requester_number_of_posts_at_retrieval", "requester_number_of_posts_on_raop_at_request" ,
"requester_number_of_posts_on_raop_at_retrieval" , "requester_number_of_subreddits_at_request" ,
"requester_upvotes_minus_downvotes_at_request", "requester_upvotes_minus_downvotes_at_retrieval",
"requester_upvotes_plus_downvotes_at_request"  , "requester_upvotes_plus_downvotes_at_retrieval",
"requester_number_of_comments_at_retrieval", "requester_number_of_comments_in_raop_at_request",
"request_id")
train[, numNames] = lapply(train[,numNames],as.numeric)
str(train)
charNames = c("giver_username_if_known", "request_text", "request_text_edit_aware",
"request_title", "requester_subreddits_at_request", "requester_username")
train[,charNames] = lapply(train[,charNames], as.character)
str(train)
numNamesTest = c("requester_account_age_in_days_at_request", "requester_days_since_first_post_on_raop_at_request",
"requester_number_of_comments_at_request","requester_number_of_comments_in_raop_at_request",
"requester_number_of_posts_at_request","requester_number_of_posts_on_raop_at_request",
"requester_number_of_subreddits_at_request","requester_upvotes_minus_downvotes_at_request",
"requester_upvotes_plus_downvotes_at_request")
test[, numNamesTest] = lapply(test[, numNamesTest], as.numeric)
charNamesTest = c("request_id", "giver_username_if_known" , "request_text_edit_aware", "request_title",
"requester_subreddits_at_request", "requester_username")
test[, charNamesTest] = lapply(test[, charNamesTest], as.character)
#Simple Log Model w/o text variables
simpleModel = glm(requester_received_pizza ~
requester_account_age_in_days_at_request + requester_days_since_first_post_on_raop_at_request +
requester_number_of_comments_at_request + requester_number_of_comments_in_raop_at_request +
requester_number_of_posts_at_request + requester_number_of_posts_on_raop_at_request +
requester_number_of_subreddits_at_request + requester_upvotes_minus_downvotes_at_request +
requester_upvotes_plus_downvotes_at_request , data = train, family = binomial )
summary(simpleModel)
predictions = predict(simpleModel, newdata = test, type = 'response')
head(predictions)
binomialPred = as.numeric(predictions > 0.5)
head(binomialPred)
#Create bag of words
library(tm)
test$requester_received_pizza = NA
newTrain = subset(train, select = colnames(test))
newTrain$requester_received_pizza = train$requester_received_pizza
fullData = rbind(newTrain, test)
str(fullData)
textCorpus = Corpus(VectorSource(fullData$request_text_edit_aware))
textCorpus[[1]]
textCorpus = tm_map(textCorpus, tolower)
textCorpus[[1]]
textCorpus = tm_map(textCorpus, removePunctuation)
textCorpus[[1]]
textCorpus = tm_map(textCorpus, removeWords, stopwords('English'))
textCorpus[[1]]
textCorpus = tm_map(textCorpus, PlainTextDocument)
textCorpus[[1]]
textCorpus = tm_map(textCorpus, stemDocument)
textCorpus[[1]]
textDTM = DocumentTermMatrix(textCorpus)
textDTM
textDTMSparse = removeSparseTerms(textDTM, 0.95)
textDTMSparse
textTerms = as.data.frame(as.matrix(textDTMSparse))
colnames(textTerms) = paste0('text_', colnames(textTerms))
colnames(textTerms)
str(textTerms)
subredditCorpus = Corpus(VectorSource(fullData$requester_subreddits_at_request))
subredditCorpus[[1]]
subredditCorpus = tm_map(subredditCorpus, tolower)
subredditCorpus[[1]]
subredditCorpus = tm_map(subredditCorpus, PlainTextDocument)
subredditCorpus[[1]]
subredditCorpus = tm_map(subredditCorpus, removePunctuation)
subredditCorpus[[1]]
subredditCorpus = tm_map(subredditCorpus, removeWords, stopwords('English'))
subredditCorpus[[1]]
subredditCorpus = tm_map(subredditCorpus, stemDocument)
subredditCorpus[[1]]
subredditDTM = DocumentTermMatrix(subredditCorpus)
subredditDTM
subredditDTMSparse = removeSparseTerms(subredditDTM, 0.95)
subredditDTMSparse
subredditTerms = as.data.frame(as.matrix(subredditDTMSparse))
colnames(subredditTerms) = paste0('subreddit_', colnames(subredditTerms))
colnames(subredditTerms)
str(subredditTerms)
titleCorpus = Corpus(VectorSource(fullData$request_title))
titleCorpus[[1]]
titleCorpus = tm_map(titleCorpus, tolower)
titleCorpus[[1]]
titleCorpus = tm_map(titleCorpus, PlainTextDocument)
titleCorpus[[1]]
titleCorpus = tm_map(titleCorpus, removePunctuation)
titleCorpus[[1]]
titleCorpus = tm_map(titleCorpus, removeWords, stopwords('English'))
titleCorpus[[1]]
titleCorpus = tm_map(titleCorpus, stemDocument)
titleCorpus[[1]]
titleDTM = DocumentTermMatrix(titleCorpus)
titleDTM
titleDTMSparse = removeSparseTerms(titleDTM, 0.95)
titleDTMSparse
titleTerms = as.data.frame(as.matrix(titleDTMSparse))
colnames(titleTerms) = paste0('title_', colnames(titleTerms))
colnames(titleTerms)
str(titleTerms)
trainBagOfWords = cbind(textTerms, subredditTerms, titleTerms, row.names = NULL)
str(trainBagOfWords)
head(trainBagOfWords)
trainSub = subset(fullData, select = c("requester_account_age_in_days_at_request",
"requester_days_since_first_post_on_raop_at_request",
"requester_number_of_comments_at_request",
"requester_number_of_comments_in_raop_at_request",
"requester_number_of_posts_at_request",
"requester_number_of_posts_on_raop_at_request",
"requester_number_of_subreddits_at_request",
"requester_upvotes_minus_downvotes_at_request",
"requester_upvotes_plus_downvotes_at_request",
"requester_received_pizza"))
newData = cbind(trainBagOfWords, trainSub)
colnames(newData)
str(newData)
newTrain = subset(newData, newData$requester_received_pizza != NA)
newTest = subset(newData, newData$requester_received_pizza == NA)
str(newTrain)
unique(newData$requester_received_pizza)
newTrain = subset(newData, newData$requester_received_pizza != NA)
str(newTrain)
newTest = subset(newData, newData$requester_received_pizza == NA)
str(newTest)
newTrain = subset(newData, !is.na(requester_received_pizza))
str(newTrain)
newTest = subset(newData, is.na(requester_received_pizza))
str(newTest)
newTest$requester_received_pizza = NULL
logModel = glm(requester_received_pizza ~ ., data = newTrain, family = binomial)
summary(logModel)
summary(simpleModel)
predictions = predict(logModel, newdata = newTest, type = 'response')
head(predictions)
binomialPred = predictions > 0.5
binomialPred = as.numeric(predictions > 0.5)
unique(binomialPred)
submission = data.frame(request_id = test$request_id, requester_received_pizza = binomialPred)
write.csv(submission, 'BagOfWordsLogModel.csv', row.names = FALSE)
?rpart
library(rpart)
library(rpart.plot)
pizzaCART = rpart(requester_received_pizza ~ ., data = newTrain, method = 'class')
predictions = predict(pizzaCART, newdata = newTest, type = 'response')
predictions = predict(pizzaCART, newdata = newTest)
head(predictions)
binomialPred = as.numeric(predictions[, 2] > 0.5)
unique(binomialPred)
head(binomialPred)
unique(binomialPred)
submission = data.frame(request_id = test$request_id, requester_received_pizza = binomialPred)
write.csv(submission, 'BagOfWordsCARTModel.csv', row.names = FALSE)
library(rpart)
library(rpart.plot)
library(caret)
library(e1071)
# Define cross-validation experiment
numFolds = trainControl( method = "cv", number = 10 )
cpGrid = expand.grid( .cp = seq(0.01,0.5,0.01))
train(requester_received_pizza ~ ., data = newTrain, method = "rpart", trControl = numFolds, tuneGrid = cpGrid)
pizzaCART = rpart(requester_received_pizza ~ ., data = newTrain, method = 'class', cp = 0.01)
predictions = predict(pizzaCART, newdata = newTest)
head(predictions)
binomialPred = as.numeric(predictions[, 2] > 0.5)
head(binomialPred)
unique(binomialPred)
submission = data.frame(request_id = test$request_id, requester_received_pizza = binomialPred)
write.csv(submission, 'BagOfWordsCARTModel.csv', row.names = FALSE)
library(randomForest)
?tuneRF
seq(0.01,0.5,0.01)
numFolds = trainControl( method = "cv", number = 10 )
cpGrid = expand.grid( .cp = seq(0.01,0.5,0.001))
tr = train(requester_received_pizza ~ ., data = newTrain, method = "rpart", trControl = numFolds, tuneGrid = cpGrid)
bestTree = tr$finalModel
predictions = predict(bestTree, newdata = newTest)
head(predictions)
binomialPred = as.numeric(predictions[, 2] > 0.5)
head(binomialPred)
unique(binomialPred)
predictions = predict(tr, newdata = newTest)
head(predictions)
predictions = as.numeric(predict(tr, newdata = newTest))
head(predictions)
unique(predictions)
predictions = predict(tr, newdata = newTest)
head(predictions)
unique(predictions)
names(tr)
tr$bestTine
?tuneRF
optimumMTry = tuneRF(requester_received_pizza ~ ., data = newTrain, ntreeTry = 500)
optimumMTry = tuneRF(x = newTrain[, - c('requester_received_pizza')],
y = newTrain[, c('requester_received_pizza')], ntreeTry = 500)
optimumMTry = tuneRF(x = newTrain[, ! c('requester_received_pizza')],
y = newTrain[, c('requester_received_pizza')], ntreeTry = 500)
colnames(newTrain)
optimumMTry = tuneRF(x = newTrain[, c(1:201)],
y = newTrain[, c('requester_received_pizza')], ntreeTry = 500)
pizzaRF = randomForest(requester_received_pizza ~ ., data = newTrain, mtry = 14, nodesize = 25,
ntree = 500)
predictions = predict(pizzaRF, newdata = newTest)
head(predictions)
RFBinomialPred = as.numeric(predictions)
head(RFBinomialPred)
unique(RFBinomialPred)
head(predictions)
class(predictions[1])
RFBinomialPred = as.numeric(as.logical(predictions))
head(RFBinomialPred)
unique(RFBinomialPred)
submission = data.frame(request_id = test$request_id, requester_received_pizza = binomialPred)
write.csv(submission, 'BagOfWordsRFModel.csv', row.names = FALSE)
logModel = glm(requester_received_pizza ~ ., data = newTrain, family = binomial)
summary(logModel)
predictions = predict(logModel, newdata = newTest, type = 'response')
head(predictions)
LogBinomialPred = as.numeric(predictions > 0.5)
unique(LogBinomialPred)
submission = data.frame(request_id = test$request_id, requester_received_pizza = binomialPred)
write.csv(submission, 'BagOfWordsLogModel.csv', row.names = FALSE)
predictions = predict(pizzaRF, newdata = newTest, type = response)
predictions = predict(pizzaRF, newdata = newTest, type = 'response')
head(predictions)
RFBinomialPred = as.numeric(as.logical(predictions))
head(RFBinomialPred)
unique(RFBinomialPred)
submission = data.frame(request_id = test$request_id, requester_received_pizza = binomialPred)
predictions = (LogBinomialPred + RFBinomialPred) / 2
head(predictions)
unique(predictions)
binomialPred = predictions >= 0.5
unique(binomialPred)
binomialPred = as.numeric(predictions >= 0.5)
unique(binomialPred)
submission = data.frame(request_id = test$request_id, requester_received_pizza = binomialPred)
write.csv(submission, 'BagOfWordsRF+LogEnsemble.csv', row.names = FALSE)
colnames(test)
logModel1 = glm( requester_received_pizza ~ giver_username_if_known, data = train, family = binomial )
predictions = predict(logModel1, newdata = test, type = 'response')
logModel1 = glm( requester_received_pizza ~ giver_username_if_known, data = fullData, family = binomial )
summary(logModel1)
predictions = predict(logModel1, newdata = test, type = 'response')
nrow(fullData)
nrow(test) + nrow(train)
head(test)
head(test$giver_username_if_known)
unique(test$giver_username_if_known)
