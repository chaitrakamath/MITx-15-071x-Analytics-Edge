+           }
+      }
for (j in 1:k){
best.fit = regularsubsets(Salary~., data = Hitters[folds != j, ], nvmax = 19)
for (i in 1:19){
pred = predict(best.fit, Hitters[folds == j,], id = i)
cv.errors[j,i] = mean((Hitters$Salary[folds == j] - pred) ^2)
}
}
for (j in 1:k){
best.fit = regsubsets(Salary~., data = Hitters[folds != j, ], nvmax = 19)
for (i in 1:19){
pred = predict(best.fit, Hitters[folds == j,], id = i)
cv.errors[j,i] = mean((Hitters$Salary[folds == j] - pred) ^2)
}
}
install.packages('glmnet')
library(glmnet)
dim(Hitters)
model.matrix(Salary~., Hitters)
x = model.matrix(Salary~., Hitters)[,-1]
head(x)
y = Hitters$Salary
grid=10^seq(10,-2,length=100)
head(grid)
head((10,-2,length=100))
(10,-2,length=100)
head(seq(10,-2,length=100))
?seq
head(seq(10,-2,length=10))
head(seq(10,-2,length=25))
head(grid)
ridge.mod = glmnet(x,y,alpha = 0, lambda = grid)
seq(-2,10,length = 10)
seq(-2,10,length = 10)
seq(10,-2,length = 10)
tail(grid)
dim(coef(ridge.mod))
ridge.mod$lambda[50]
coef(ridge.mod, id = 50)
coef(ridge.mod)[50]
coef(ridge.mod)[,50]
sum(coef(ridge.mod)[,50])
sum((coef(ridge.mod)[,50])^2)
sqrt(sum((coef(ridge.mod)[,50])^2))
sqrt(sum((coef(ridge.mod)[-1,50])^2))
ridge.mod$lambda[,70]
ridge.mod$lambda[70]
coef(ridge.mod)[,70]
sqrt((coef(ridge.mod)[-1,70])^2)
sqrt(sum((coef(ridge.mod)[-1,70])^2))
predict(ridge.mod, s = 50, type = 'coefficients')[1:20,]
set.seed(1)
?sample
train = sample(1:nrow(x), nrow(x)/2)
test = !train
head(train)
head(test)
test = -train
head(test)
length(x)
length(train)
length(test)
nrow(x)
y.test = y[test]
ridge.mod = glmnet(x[train,], y[train,], alpha = 0, lambda = grid, tresh = 1e-12)
ridge.mod = glmnet(x[train,], y[train,], alpha = 0, lambda = grid, thresh = 1e-12)
ridge.mod = glmnet(x[train,], y[train], alpha = 0, lambda = grid, thresh = 1e-12)
ridge.pred = predict(ridge.mod, s = 4, newx = x[test,])
mean((ridge.pred - y.test)^2)
set.seed(1)
cv.out = cv.glmnet(x[train,], y[train], alpha = 0)
plot(cv.out)
bestlam = cv.out$lambda.min
bestlam
ridge.pred = predict(ridge.mod, s = bestlam, newx = x[test,])
mean((ridge.pred - y.test) ^ 2)
out = glmnet(x,y,alpha = 0)
predict(out, type = 'coefficients', s = bestlam)[1:20,]
lasso.mod = glmnet(x,y,alpha = 1)
lasso.mod = glmnet(x[train,],y[train],alpha = 1)
plot(lasso.mod)
set.seed(1)
cv.out = cv.glmnet(x[train,], y[train], alpha = 1)
plot(cv.out)
bestlam = cv.out$lamda.min
bestlam
cv.out=cv.glmnet(x[train ,],y[train],alpha=1)
bestlam=cv.out$lambda .min
bestlam=cv.out$lambda.min
bestlam
plot(cv.out)
cv.out = cv.glmnet(x[train,], y[train], alpha = 1)
plot(cv.out)
cv.out = cv.glmnet(x[train,], y[train], alpha=1)
set.seed(1)
cv.out = cv.glmnet(x[train,], y[train], alpha = 1)
bestlam = cv.out$lambda.min
bestlam
set.seed(1)
cv.out = cv.glmnet(x[train,], y[train], alpha = 1)
bestlam=cv.out$lambda.min
bestlam
lasso.pred = predict(lasso.mod, s = bestlam, newx = x[test,])
mean((lasso.pred - y.test) ^2)
out = glmnet(x,y,alpha = 1, lambda = grid)
lasso.coef = predict(out, type = 'coefficients', s = bestlam)[1:20,]
lasso.coef
install.packages('pls')
library(pls)
set.seed(2)
pcr.fit = pcr (Salary ~., data = Hitters, scale = TRUE, validation = 'CV')
summary(pcr.fit)
validationplot(pcr.fit, type = 'MSEP')
validationplot(pcr.fit, val.type = 'MSEP')
set.seed (1)
pcr.fit = pcr(Salary~., data = Hitters, subset = train, scale = TRUE)
pcr.fit = pcr(Salary~., data = Hitters, subset = train, scale = TRUE, validation = 'CV')
validationplot(pcr.fit, val.type = 'MSEP')
pcr.pred = predict(pcr.fit, x[test,], ncomp = 7)
mean((pcr.pred - y.test) ^2)
pcr.fit = pcr(y~x, scale = TRUE, ncomp = 7)
summary(pcr.fit)
names(cv.out)
?cv.glmnet
library(tree)
install.packages('tree')
library(tree)
library(ISLR)
attach(Carseats)
High = ifelse(Sales >= 0.8, 'Yes', 'No')
str(Carseats)
Carseats = data.frame(Carseats, High)
str(Carseats)
tree.carseats = tree (High ~ .-Sales, data = Carseats)
summary(tree.carseats)
plot(tree.carseats)
text (tree.carseats)
text (tree.carseats, pretty = 0)
High = ifelse(Sales < 0.8, 'No', 'Yes')
tree.carseats = tree (High ~ .-Sales, data = Carseats)
summary(tree.carseats)
plot(tree.carseats)
text (tree.carseats)
plot(tree.carseats, pretty = 0)
text (tree.carseats, pretty = 0)
names(Carseats)
rm('Carseats')
Carseats
rm(Carseats)
ls()
rm(list = ls())
ls()
Carseats
rm(Carseats, inherits = TRUE)
ls()
str(Carseats)
ls()
High
High = ifelse(Sales <  0.8, 'No', 'Yes')
High
High = ifelse(Sales <  8, 'No', 'Yes')
High
str(Carseats)
Carseats = data.frame(Carseats, High)
str(Carseats)
tree.carseats = tree(High ~ . - Sales, data = Carseats)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats)
unique(Carseats$ShelveLoc)
text(tree.carseats, pretty = 0)
tree.carseats
set.seed(2)
train = sample(1: nrow(Carseats),200)
Carseats.test = Carseats[-train, ]
High.test = High[-train, ]
High.test = High[-train]
head(High)
tree.carseats = tree(High ~ . - Sales, data = Carseats, subset = train)
tree.pred = predict(tree.carseats, Carseats.test, type = 'class')
table(tree.pred)
table(tree.pred, High.test)
set.seed(3)
cv.carseats = cv.tree(tree.carseats, FUN = prune.misclass)
names(cv.carseats)
cv.carseats
part(mfrow = c(1, 2))
par(mfrow = c(1, 2))
plot(cv.carseats$size, cv.carseats$dev, type = 'b')
plot(cv.carseats$k, cv.carseats$dev, type = 'b')
prune.carseats = prune.misclass(tree.carseats, best = 9)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
tree.pred = predict(prune.carseats, Carseats.test, type = 'class')
table(tree.pred, Carseats.test)
table(tree.pred, High.test)
prune.carseats = prune.misclass(tree.carseats, best = 15)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
tree.pred = predict(prune.carseats, Carseats.test, type = 'class')
table(tree.pred, High.test)
library(MASS)
set.seed(1)
train = sample(1:nrow(Boston), nrow(Boston) / 2)
len(train)
str(train)
str(Boston)
length(train)
tree.boston = tree(medv ~ ., data = Boston, subset = train)
summary(tree.boston)
plot(tree.boston)
text (tree.boston, pretty = 0)
cv.boston = cv.tree(tree.boston)
cv.boston
plot(cv.boston$dev ~ cv.boston$size, type = 'b')
prune.boston = prune.tree(tree.boston, best = 8)
prune.boston = prune.tree(tree.boston, best = 5)
plot(prune.boston)
text(prube.boston, pretty = 0)
text(prune.boston, pretty = 0)
yhat = predict(tree.boston, newdata = Boston[-train])
yhat = predict(tree.boston, newdata = Boston[-train,])
boston.test = Boston[-train, 'medv']
plot(yhat, boston.test)
abline(o,1)
abline(0,1)
library(randomForest)
install.packages('randomForests')
library(randomForest)
install.packages('randomForest')
library(randomForest)
set.seed(1)
bag.boston = randomForest(medv ~ ., data = Boston, subset = train, mtry = 13, importance = TRUE)
bag.boston
?randomForest()
str(Boston)
yhat.bag = predict(bag.boston, newdata = Boston[-train,])
plot(yhat.bag, boston.test)
abline(0,1)
bag.boston = randomForest(medv ~ ., data = Boston, subset = train, mtry = 13, importance = TRUE, ntree = 25)
bag.boston
set.seed(1)
rf.boston = randomForest(medv ~ ., data = Boston, mtry = 6, importance = TRUE)
rf.boston = randomForest(medv ~ ., data = Boston, subset = train, mtry = 6, importance = TRUE)
rf.boston
yhat.rf = predict(rf.boston, newdata = Boston[-train,])
mean((yhat.rf - boston.test) ^ 2)
importance (bag.boston)
varImpPlot(rf.boston)
install.packages('gbm')
library(gbm)
set.seed(1)
boost.boston = gbm(medv ~ ., data = Boston[train, ], ntrees = 5000, distribution = 'gaussian', interaction.depth = 4)
boost.boston = gbm(medv ~ ., data = Boston[train, ], n.trees = 5000, distribution = 'gaussian', interaction.depth = 4)
boost.boston = gbm(medv ~ ., data = Boston, subset = train, n.trees = 5000, distribution = 'gaussian', interaction.depth = 4)
boost.boston = gbm(medv ~ ., data = Boston[train, ], n.trees = 5000, distribution = 'gaussian', interaction.depth = 4)
summary(boost.boston)
par(mfrow=c(1,2))
plot(boost.boston, i = 'rm')
plot(boost.boston, i = 'lstat')
yhat.boost = predict(boost.boston, newdata = Boston[-train,], n.trees = 5000)
mean((yhat.boston - Boston[-train, medv]) ^ 2)
mean((yhat.boost - Boston[-train, medv]) ^ 2)
mean((yhat.boost - Boston[-train, 'medv']) ^ 2)
boost.boston = gbm(medv ~ ., data = Boston[train, ], n.trees = 5000, distribution = 'gaussian', interaction.depth = 4, shrinkage = 0.2, verbose = False)
boost.boston = gbm(medv ~ ., data = Boston[train, ], n.trees = 5000, distribution = 'gaussian', interaction.depth = 4, shrinkage = 0.2, verbose = FALSE)
yhat.boost = predict(boost.boston, newdata = Boston[-train,], n.trees = 5000)
mean((yhat.boost - Boston[-train, 'medv']) ^ 2)
install.packages ('boot')
library(boot)
sample(3, replace = T)
set.seed(1)
x = matrix(rnorm(20 * 2), ncol = 2)
x
y = c(rep(-1, 10), rep (1,10))
y
x[y == 1, ] = x[y == 1, ] + 1
x
plot(x, col = (3 - y))
plot (x)
plot(x, col = (3 - y))
dat = data.frame(x, y = as.factor(y))
install.packages('e1071')
library(e1071)
svmfit = svm (y ~ ., data = dat, cost = 10, kernel = 'linear', scale = FALSE)
plot(svmfit, dat)
plot.svm(svmfit, dat)
dat
names(svmfit)
svmfit$index
summary(svmfit)
svmfit = svm (y ~ ., data = dat, cost = 0.1, kernel = 'linear', scale = FALSE)
plot(svmfit, dat)
svmfit$index
summary(svmfit)
set.seed(1)
tune.out = tune(svm, y ~ ., data = dat, kernel = 'linear', ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100,)))
tune.out = tune(svm, y ~ ., data = dat, kernel = 'linear', ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
tune.out=tune(svm,yâˆ¼.,data=dat,kernel="linear",              ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)))
tune.out=tune(svm,yâˆ¼.,data=dat,kernel="linear",ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)))
tune.out=tune(svm,y ~.,data=dat,kernel="linear",ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)))
?tune()
tune.out = tune(svm, y~., data = dat, ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)), kernel = 'linear')
tune(svm, Species~., data = iris, ranges = list(gamma = 2^(-1:1), cost = 2^(2:4)),tunecontrol = tune.control(sampling = "fix"))
xtest = matrix(rnorm(20 * 2), ncol = 2)
ytest = sample(c(-1, 1), 20, rep = TRUE)
xtest[ytest == 1, ] = xtest[ytest == 1, ] + 1
testdat = data.frame(xtest, ytest = as.factor(ytest))
bestmod = svm(y ~ ., data = dat, kernel = 'linear', cost = 0.1)
ypred = predict(bestmod, newdata = testdat)
ypred
table(predict = ypred, truth = testdat$ytest)
bestmod = svm(y ~ ., data = dat, kernel = 'linear', cost = 0.1, scale = FALSE)
ypred = predict(bestmod, newdata = testdat)
table(predict = ypred, truth = testdat$ytest)
svmfit = svm(y ~ ., data = dat, kernel = 'linear', cost = 0.01, scale = FALSE)
set.seed(1)
bestmod = svm(y ~ ., data = dat, kernel = 'linear', cost = 0.1, scale = FALSE)
xtest = matrix(rnorm(20 * 2), ncol = 2)
ytest = sample(c(-1, 1), 20, rep = TRUE)
xtest[ytest == 1, ] = xtest[ytest == 1, ] + 1
bestmod = svm(y ~ ., data = dat, kernel = 'linear', cost = 0.1, scale = FALSE)
table(predict = ypred, truth = testdat$ytest)
ypred = predict(svmfit, newdata = testdat)
table(ypred, testdat$ytest)
x[y == 1, ] = x[y == 1, ] + 0.5
plot(x, col = (y + 5) / 2, pch = 19)
dat = data.frame(x, y = as.factor(y))
svmfit = svm(y ~ ., data = dat, kernle = 'linear', cost = 1e5)
summary(svmfit)
plot(svmfit, dat)
svmfit = svm(y ~ ., data = dat, kernle = 'linear', cost = 1)
summary(svmfit)
plot(svmfit, dat)
set.seed(1)
x = matrix(rnorm(20 * 2), ncol = 2)
set.seed(1)
x = matrix(rnorm(200 * 2), ncol = 2)
x[1:100, ] = x[1:100, ] + 2
x[101:150, ] = x[101:150, ] - 2
?sample()
y = c(rep(1, 150), rep(2, 50))
dat = data.frame(x, y = as.factor(y))
plot(x, col = y)
train = sample(200, 100)
svmfit = svm (y ~ ., data = dat[train,], kernel = 'radial', gamma = 1, cost = 1)
plot(svmfit, dat[train,])
summary(svmfit)
svmfit = svm (y ~ ., data = dat[train,], kernel = 'radial', gamma = 1, cost = 1e5)
plot(svmfit, dat[train,])
summary(svmfit)
set.seed(1)
tune.out = tune(svm, y~., data = dat[train,], ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)), gamma = c(0.5, 1, 2, 3, 4), kernel = 'radial')
set.seed(1)
tune.out=tune(svm, y~., data=dat[train,], kernel="radial",ranges=list(cost=c(0.1,1,10,100,1000),gamma=c(0.5,1,2,3,4) ))
tune.out=tune(svm, y~., data=dat[train,], kernel="radial",ranges=list(cost=c(0.1,1,10,100,100),gamma=c(0.5,1,2,3,4)))
set.seed(1)
bestmod = svm(y ~ ., data = dat[train,], cost = 1, gamma = 2, kernel = 'radial')
table(pred = predict(bestmod, newdata =dat[-train,]), true = dat[-train, 'y'])
library(ROCR)
install.packages('ROCR')
library(ROCR)
rocplot = function(pred, truth, ...){
predob = predict(pred, truth)
}
rocplot = function(pred, truth, ...){
predob = prediction(pred, truth)
perf = performance(predob, 'tpr', 'fpr')
plot(perf, ...)
}
svmfit.opt = svm (y ~ ., data = dat[train,], kernel = 'radial', gamma = 2, cost = 1, decision.values = TRUE)
fitted = attributes(svmfit.opt, newdata = dat[-train,], decision.values = TRUE)
fitted = attributes(predict(svmfit.opt, newdata = dat[-train,], decision.values = TRUE))
names(fitted)
fitted$decision.values
fitted$class
names(fitted)
fitted$levels
fitted$names
par(mfrow = c(1,2))
rocplot(fitted, dat[train, 'y'], main = 'Training data')
rocplot(fitted ,dat[train ,"y"],main="Training Data")
rocplot=function(pred, truth, ...){    predob = prediction (pred, truth)    perf = performance (predob , "tpr", "fpr") plot(perf ,...)}
rocplot=function(pred, truth, ...){predob = prediction (pred, truth)    perf = performance (predob , "tpr", "fpr") plot(perf ,...)}
rocplot=function(pred, truth, ...){
predob = prediction (pred, truth)
perf = performance (predob , "tpr", "fpr")
plot(perf ,...)
}
rocplot(fitted, dat[train, 'y'], main = 'Training data')
fitted$decision.values
length(fitted$decision.values)
length(dat[train, 'y'])
str(dat)
dat$y = as.numeric(levels(dat$y))
str(dat)
rocplot(fitted, dat[train, 'y'], main = 'Training data')
str(dat[train, 'y'])
str(fitted$decision.values)
as.numeric(fitted$decision.values)
str(as.numeric(fitted$decision.values))
rocplot(as.numeric(fitted$decision.values), dat[train, 'y'], main = 'Training data')
dat$y = as.factor(dat$y)
str(dat)
svmfit.flex = svm(y ~ ., data = dat[train, ], kernel = 'radial', gamma = 50, cost = 1, decision.values = T)
fitted = attributes(predict(svmfit.flex, newdata = dat[-train, y], decision.values = T))$decision.values
rocplot(fitted, dat[train, 'y'], add = 'T', col = 'red')
svmfit.opt = svm(y ~ ., data = dat[train,], kernel = 'radial', cost = 1, gamma = 2)
svmfit.opt = svm(y ~ ., data = dat[train,], kernel = 'radial', cost = 1, gamma = 2, decision.values = T)
fitted = attributes(predict(svmfit.opt, x[-train, 'y'], decision.values = T))$decision.values
fitted = attributes(predict(svmfit.opt, x[-train,], decision.values = T))$decision.values
rocplot(fitted, dat[-train, 'y'], main = 'Test Data')
fitted = attributes(predict(svmfit.flex, x[-train,], decision.values = T))$decision.values
rocplot(fitted, dat[-train, 'y'], add = 'T', col = 'red')
set.seed(1)
x = rbind(x, matrix(rnorm(50)))
x = rbind(x, matrix(rnorm(50*2), ncol = 2))
str(x
)
x
class(x)
y = c(y, rep(0, 50))
x[y == 0, ] = x[y == 0, ] + 2
dat = data.frame(x, as.factor(y))
par(mfrow= c(1, 2))
par(mfrow= c(1, 1))
plot(x, col = y)
plot(x, col = (y + 1))
plot(x, col = (y + 1))
plot(x, col = (y + 1))
plot(x, col = y)
plot(x, col = (y + 1))
smvfit = svm(y ~ ., data = dat, kernel = 'radial', cost = 10, gamma = 1)
plot(svmfit, dat)
library(ISLR)
names(Khan)
dim(xtrain)
dim(Khan$xtrain)
class(Khan)
class(Khan$xtrain)
dim(Khan$xtest)
length(Khan$ytrain)
length(Khan$ytest)
data = data.frame(x = Khan$xtrain, y = as.factor(Khan$ytrain))
head(data)
dat = data.frame(x = Khan$xtrain, y = as.factor(Khan$ytrain))
out = svm(ytrain ~ ., data = dat, kernel = 'linear', cost = 10)
out = svm(y ~ ., data = dat, kernel = 'linear', cost = 10)
summary(out)
plot(out, dat)
table(out$fitted, dat$y)
dat.test = data.frame(x = Khan$xtest, y = as.factor(Khan$ytest))
pred.test = predict(out, newdata = dat.test)
table(pred.test, dat.test$y)
rnorm(10, mean = 0)
rnorm(10, mean = 0, sd = diag(10))
rnorm(10, mean = 0, sd = diag(10))
rnorm(10, mean = 0, sd = diag(10))
rnorm(10, mean = 0, sd = diag(10))
rnorm(10, mean = 0, sd = diag(10))
matrix(rnorm(10, mean = 0, sd = diag(10),  10, 10)
)
matrix(rnorm(10, mean = 0, sd = diag(10)),  10, 10)
matrix(rnorm(100, mean = 0, sd = diag(10)),  10, 10)
diag(3)
matrix(rnorm(100, mean = rep(0,10), sd = diag(10)),  10, 10)
rep(0,10)
x2 = matrix(rnorm(100, mean = c(1,1,1,1,1,0,0,0,0,0), sd = diag(10)), 10, 10)
x2
y=rep(c(-1,1),c(10,10))
y
x1 = matrix(rnorm(100, mean = rep(0,10), sd = diag(10)),  10, 10)
x2 = matrix(rnorm(100, mean = c(1,1,1,1,1,0,0,0,0,0), sd = diag(10)), 10, 10)
x1
x2
rbind(x1, x2)
getwd()
setwd('/Users/admin/Documents/Coursera/AnalyticsEdge-edX/Week6/Recitation/')
getwd()
healthy = read.csv('healthy.csv', header = FALSE)
healthyMatrix = as.matrix(healthy)
str(healthyMatrix)
image(healthyMatrix, axes = FALSE, col = grey(seq(0, 1, length = 256)))
healthyVector = as.vector(healthyMatrix)
str(healthyVector)
n = length(healthyVector)
n * (n - 1) / 2 #This number is large. Hence, computing distance would throw memory error
k = 5
set.seed(1)
kmc = kmeans(healthyVector,centers = k, iter.max = 1000)
str(kmc)
healthyClusters = kmc$cluster
kmc$centers[2]
dim(healthyClusters) = c(nrow(healthyMatrix), ncol(healthyMatrix))
image(healthyClusters, axes = FALSE, col = rainbow(k))
tumor = read.csv('tumor.csv', header = FALSE)
tumorMatrix = as.matrix(tumor)
tumorVector = as.vector(tumorMatrix)
install.packages('flexClust')
install.packages('flexclust')
library(flexclust)
kmc.kcca = as.kcca (kmc, healthyVector)
tumorClusters = predict(kmc.kcca, newdata = tumorVector)
dim(tumorClusters) = c(nrow(tumorMatrix), ncol(tumorMatrix))
image(tumorClusters, axes = FALSE, col = rainbow(k))
getwd()
setwd('/Users/admin/Documents/Coursera/AnalyticsEdge-edX/Week6/Assignment/')
getwd()
dailykos = read.csv('dailykos.csv')
str(dailykos)
distance = dist(dailykos, method = 'euclidean')
