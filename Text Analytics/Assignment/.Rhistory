library(SnowballC)
corpus = Corpus(VectorSource(tweets$Tweet))
corpus
corpus[[1]]
corpus = tm_map (corpus, tolower)
corpus[[1]]
corpus = tm_map (corpus, removePunctuation)
corpus[[1]]
stopWords('English')[1:10]
stopwords('English')[1:10]
corpus = tm_map (corpus, removeWords, c('apple', stopwords('English')))
corpus[[1]]
corpus = tm_map (corpus, stemDocument)
corpus[[1]]
corpus = Corpus(VectorSource(tweets$Tweet))
corpus
corpus[[1]]
corpus = tm_map (corpus, tolower)
corpus[[1]]
corpus = tm_map(corpus, PlainTextDocument)
corpus = tm_map (corpus, removePunctuation)
corpus[[1]]
stopwords('English')[1:10]
corpus = tm_map (corpus, removeWords, c('apple', stopwords('English')))
corpus[[1]]
corpus = tm_map (corpus, stemDocument)
corpus[[1]]
frequencies = DocumentTermMatrix(corpus)
frequencies
inspect(frequencies[1000:1005, 505:515])
findFreqTerms(frequencies, lowFreq = 20)
findFreqTerms(frequencies, lowfreq = 20)
sparse = removeSparseTerms(frequencies, 0.005)
sparse
sparse = removeSparseTerms(frequencies, 0.995)
sparse
tweets.sparse = as.data.frame(as.matrix(sparse))
colnames(tweets.sparse) = make.names(colnames(tweets.sparse))
tweet.sparse$Negative = tweets$Negative
tweets.sparse$Negative = tweets$Negative
library(caTools)
set.seed(123)
spl = sample.split(tweets.sparse$Negative, SplitRatio = 0.7)
library(caTools)
set.seed(123)
spl = sample.split(tweets.sparse$Negative, SplitRatio = 0.7)
library(caTools)
set.seed(123)
spl = sample.split(tweets.sparse$Negative, SplitRatio = 0.7)
tweets.sparse$Negative[1:10]
tweets.sparse$Negative = tweetsNegative
library(caTools)
set.seed(123)
spl = sample.split(tweets.sparse$Negative, SplitRatio = 0.7)
trainSparse = subset(tweet.sparse, spl == TRUE)
library(caTools)
set.seed(123)
spl = sample.split(tweets.sparse$Negative, SplitRatio = 0.7)
trainSparse = subset(tweets.sparse, spl == TRUE)
testSparse = subset(tweets.sparse, spl == FALSE)
findFreqTerms(frequencies, lowfreq = 100)
library(rpart)
library(rpart.plot)
tweetCART = rpart(Negative ~ ., data = trainSparse, method = 'class')
prp(tweetCART)
predictCART = predict(tweetCART, newdata = testSparse, type = 'class')
table(testSparse$Negative, predictCART)
accuracy = (294 + 18) / nrow(testSparse)
accuracy
table(testSparse$Negative)
baselineAccuracy = 300 / nrow(testSparse)
baselineAccuracy
library(randomForest)
set.seed(123)
tweetRF = randomForest(Negative ~ ., data = trainSparse)
predictRF = predict(tweetRF, newdata = testSparse)
table(testSparse$Negative, predictRF)
rfAccuracy = (293 + 21) / nrow(testSparse)
rfAccuracy
logModel = glm(Negative ~ ., data = trainSparse, family = 'binomial')
predictions = (logModel, newdata = testSparse, type = 'response')
predictions = predict(logModel, newdata = testSparse, type = 'response')
tweetLog = glm(Negative ~ ., data = trainSparse, family = 'binomial')
predictions = predict(tweetLog, newdata = testSparse, type = 'response')
table(testSparse$Negative, predictions)
table(testSparse$Negative, predictions > 0.5)
logAccuracy = (253 + 32) / nrow(testSparse)
logAccuracy
setwd('/Users/admin/Documents/Coursera/AnalyticsEdge-edX/Week5/Lecture')
emails = read.csv('energy_bids.csv', stringsAsFactors = FALSE)
str(emails)
email$email[1]
emails$email[1]
strwrap(emails$email[1])
emails$responsive[1]
strwrap(emails$email[2])
emails$responsive[2]
table(emails$responsive)
library(tm)
corpus = Corpus(VectorSource(emails$email))
strwrap(corpus[[1]])
corpus = tm_map (corpus, tolower)
corpus = tm_map (corpus, PlainTextDocument)
corpus = tm_map (corpus, removePunctuation)
corpus = tm_map (corpus, removeWords, stopwords('English'))
corpus = tm_map (corpus, stemDocument)
strwrap(corpus[[1]])
dtm = DocumentTermMatrix(corpus)
dtm
dtm = removeSparseTerms(dtm, 0.97)
dtm
labelledTerms = as.data.frame(as.matrix(dtm))
labelledTerms$Responsive = emails$responsive
str(labelledTerms)
library(caTools)
set.seed(144)
spl = sample.split(labelledTerms$Responsive, SplitRatio = 0.7)
train = subset(labelledTerms, spl == TRUE)
test = subset(labelledTerms, spl == FALSE)
library(rpart)
library(rpart.plot)
emailCART = rpart(Responsive ~ ., data = train, method = 'class')
prp(emailCART)
pred = predict(emailCART, newdata = test)
pred[1:10,]
pred.prob = pred[,2]
table(test$Responsive, pred.prob > 0.5)
accuracy = (195 + 25) / nrow(test)
accuracy
table(test$Responsive)
baselineAccuracy = 215 / nrow(test)
baselineAccuracy
library(ROCR)
predROCR = predict(pred.prob, test$Responsive)
predROCR = prediction(pred.prob, test$Responsive)
perf = performance(predROCR, 'tpr', 'fpr')
plot(perf, colorize = TRUE)
performance(pred, 'auc')@y.values
performance(predROCR, 'auc')@y.values
setwd('/Users/admin/Documents/Coursera/AnalyticsEdge-edX/Week5/Assignment')
wiki = read.csv('wiki.csv', stringsAsFactors = FALSE)
str(wiki)
wiki$Vandal = as.factor(wiki$Vandal)
table(wiki$Vandal)
library(tm)
corpusAdded = Corpus(VectorSource(wiki$Added))
strwrap(corpusAdded[[1]])
corpusAdded = tm_map (corpusAdded, removeWords, stopwords('English'))
corpusAdded = tm_map (corpusAdded, stemDocument)
strwrap(corpusAdded[[1]])
dtmAdded = DocumentMatrix(corpusAdded)
dtmAdded = DocumentTermMatrix(corpusAdded)
dtmAdded
sparseAdded = removeSparseTerms(dtmAdded, 0.97)
sparseAdded
sparseAdded = removeSparseTerms(dtmAdded, 0.3)
sparseAdded
sparseAdded = removeSparseTerms(dtmAdded, 0.97)
sparseAdded
?removeSparseTerms
sparseAdded = removeSparseTerms(dtmAdded, 0.997)
sparseAdded
wordsAdded = as.data.frame(as.matrix(sparseAdded))
colnames(wordsAdded)
colnames(wordsAdded) = paste('A', colnames(wordsAdded))
colnames(wordsAdded)
names(corpusAdded)
str(corpusAdded)
names(wiki)
corpusRemoved = Corpus(VectorSource(wiki$Removed))
strwrap(corpusRemoved[[1]])
strwrap(corpusRemoved[[5]])
corpusRemoved = tm_map (corpusRemoved, removeWords, stopwords('English'))
corpusRemoved = tm_map (corpusRemoved, stemDocument)
strwrap(corpusRemoved[[5]])
dtmRemoved = DocumentTermMatrix(corpusRemoved)
dtmRemoved
sparseRemoved = removeSparseTerms(dtmRemoved, 0.997)
sparseRemoved
wordsRemoved = as.data.frame(as.matrix(sparseRemoved))
colnames(wordsRemoved) = paste('R', colnames(wordsRemoved))
nrow(wordsRemoved)
ncol(wordsRemoved)
wiki = cbind(wordsAdded, wordsRemoved)
wikiWords = cbind(wordsAdded, wordsRemoved)
wiki = read.csv('wiki.csv', stringsAsFactors = FALSE)
str(wiki)
wiki$Vandal = as.factor(wiki$Vandal)
#1.1
names(wikiWords)
str(wikiWords)
wikiWords$Vandal = wiki$Vandal
head(wikiWords$Vandal)
library(caTools)
library(caTools)
set.seed(123)
spl = sample.split(wikiWords$Vandal, SplitRatio = 0.7)
wikiTrain = subset(wikiWords, spl == TRUE)
wikiTest = subset(wikiWords, spl == FALSE)
table(wikiTest$Vandal)
baselineAccuracy = 618 / nrow(wikiTest)
baselineAccuracy
library(rpart)
library(rpart.plot)
wikiCART = rpart(Vandal ~ ., data = wikiTrain)
wikiCART = rpart(Vandal ~ ., data = wikiTrain, method = 'class')
wikiPred = predict(wikiCART, newdata = wikiTest)
table(wikiTest$Vandal, wikiPred > 0.5)
wikiPred = predict(wikiCART, newdata = wikiTest, type = 'class')
table(wikiTest$Vandal, wikiPred)
CARTAccuracy = (618 + 12) / nrow(wikiTest)
CARTAccuracy
prp(wikiCART)
wikiWords2 = wikiWords
names(wikiWords2)
wikiWords2$HTTP = ifelse(grepl ('http', wiki$Added, fixed = TRUE), 1, 0)
table(wikiWords2$HTTP)
wikiTrain2 = subset(wikiWords2, spl == TRUE)
wikiTest2 = subset(wikiWords2, spl == FALSE)
wikiCART2 = rpart(Vandal ~ ., data = wikiTrain2)
wikiPred2 = predict(wikiCART2, newdata = wikiTest2, type = 'class')
table(wikiTest2$Vandal, wikiPred2)
accuracy = (609 + 57) / nrow(wikiTest2)
accuracy
rowSums(as.matrix(dtmAdded))
?rowSums
str(iris)
head(iris)
iris.sum = colSums(iris)
iris.sum = colSums(iris[, -5])
head(iris.sum)
iris.sum
iris.sum = rowSums(iris[, -5])
head(iris.sum
)
iris.sum
wikiWords2$NumWordsAdded = rowSums(as.matrix(dtmAdded))
wikiWords2$NumWordsRemoved = rowSums(as.matrix(dtmRemoved))
tapply(wikiWords2$NumWordsAdded, mean)
mean(wikiWords2$NumWordsAdded)
wikiTrain3 = subset(wikiWords2, spl == TRUE)
wikiTest3 = subset(wikiWords2, spl == FALSE)
wikiCART3 = rpart(Vandal ~ ., data = wikiTrain3, method = 'class')
wikiPred3 = predict(wikiCART3, newdata = wikiTest3, type = 'class')
table(wikiTest3$Vandal, wikiPred3)
accuracy3 = (514 + 248) / nrow(wikiTest3)
accuracy3
wikiWords3 = wikiWords2
wikiWords3$Minor = wiki$Minor
wikiWords3$LoggedIn = wiki$Loggedin
wikiTrain4 = subset(wikiWords3, spl == TRUE)
wikiTest4 = subset(wikiWords3, spl == FALSE)
wikiCART4 = rpart(Vandal ~ ., data = wikiTrain4, method = 'class')
wikiPred4 = predict(wikiCART4, newdata = wikiTest4)
wikiPred4 = predict(wikiCART4, newdata = wikiTest4, type = 'class')
table(wikiTest4$Vandal, wikiPred4)
accuracy4 = (595 + 241) / nrow(wikiTest4)
accuracy4
prp(wikiCART4)
setwd('/Users/admin/Documents/Coursera/AnalyticsEdge-edX/Week5/Assignment')
trials = read.csv('clinical_trials.csv', stringsAsFactors = FALSE)
trials = read.csv('clinical_trial.csv', stringsAsFactors = FALSE)
nchar(trials$abstract)
max(nchar(trials$abstract))
table(nchar(trial$abstract) == 0)
table(nchar(trials$abstract) == 0)
min(nchar(trials$title))
trials[which.min(nchar(trials$title))]
trials[which.min(nchar(trials$title)), ]
library(tm)
str(trials)
corpusTitle = Corpus(trials$title)
corpusTitle = Corpus(VectorSource(trials$title))
corpusAbstract = Corpus(VectorSource(trials$abstract))
corpusTitle[[1]]
corpusTitle = tm_map(corpusTitle, tolower)
corpusTitle[[1]]
corpusTitle = tm_map(corpusTitle, PlainTextDocument)
corpusTitle[[1]]
corpusTitle = tm_map(corpusTitle, removePunctuation)
corpusTitle[[1]]
corpusTitle = tm_map(corpusTitle, removeWords, stopwords('English'))
corpusTitle[[1]]
corpusTitle = tm_map(corpusTitle, stemDocument)
corpusTitle[[1]]
corpusAbstract[[1]]
corpusAbstract[[5]]
corpusAbstract = tm_map(corpusAbstract, tolower)
corpusAbstract[[1]]
corpusAbstract[[5]]
corpusAbstract = tm_map(corpusAbstract, PlainTextDocument)
corpusAbstract[[5]]
corpusAbstract = tm_map(corpusAbstract, removePunctuation)
corpusAbstract[[5]]
corpusAbstract = tm_map(corpusAbstract, removeWords, stopwords('English'))
corpusAbstract[[5]]
corpusAbstract = tm_map(corpusAbstract, stemDocument)
corpusAbstract[[5]]
dtmTitle = DocumentTermMatrix(corpusTitle)
dtmAbstract = DocumentTermMatrix(corpusAbstract)
dtmTitle = removeSparseTerms(dtmTitle, 0.95)
dtmTitle = DocumentTermMatrix(corpusTitle)
dtmTitle
dtmTitle = removeSparseTerms(dtmTitle, 0.95)
dtmAbstract = DocumentTermMatrix(corpusAbstract)
dtmTitle
dtmAbstract = DocumentTermMatrix(corpusAbstract)
dtmAbstract
dtmAbstract = removeSparseTerms(dtmAbstract, 0.95)
dtmAbstract
dtmTitle = as.data.frame(as.matrix(dtmTitle))
dtmAbstract = as.data.frame(as.matrix(dtmAbstract))
ncol (dtmTitle)
ncol(dtmAbstract)
dtmAbstracts[, which.max(colSums(dtmAbstracts))]
dtmAbstract[, which.max(colSums(dtmAbstract))]
dtmAbstract[which.max(colSums(dtmAbstract))]
names(dtmAbstract)
nrow(dtmAbstract)
max(colSums(dtmAbstract))
which.max(colSums(dtmAbstract))
colnames(dtmTitle) = paste0('T', colnames(dtmTitle))
colnames(dtmAbstract) = paste0('A', colnames(dtmAbstract))
dtm = cbind(dtmTitle, dtmAbstract)
dtm$trial = trials$trial
ncol(dtm)
library(caTools)
set.seed(144)
spl = sample.split(dtm$trial, SplitRatio = 0.7)
train = subset(dtm, spl == TRUE)
test = subset(dtm, spl == FALSE)
table(test$trial)
baselineAccuracy = 313 / nrow(test)
baselineAccuracy
library(rpart)
library(rpart.plot)
trialCART = rpart(trial ~ ., data = train, method = 'class')
prp(trialCART)
trainPred = predict(trialCART)
head(trainPred)
max(trainPred[, 2])
table(train$trial, trainPred > 0.5)
table(train$trial, trainPred)
trainPred = predict(trialCART, type = 'class')
table(train$trial, trainPred)
trainAccuracy = (631 + 441) / nrow(train)
trainAccuracy
sensitivity = 441 / (131 + 441)
sensitivity
specificity = 631 / (631 + 99)
specificity
library(rpart)
library(rpart.plot)
pred = predict(trialCART, newdata = test, type = 'class')
table(test$trial, pred)
accuracy = (261 + 162) / nrow(test)
accuracy
library(ROCR)
pred.prob = predict(trialCART, newdata = test)
predROCR = prediction(pred.prob, test$trial)
pred.prob = predict(trialCART, newdata = test)
nrow(pred.prob)
nrow(test)
predROCR = prediction(pred.prob, test$trial)
predROCR = prediction(pred.prob[, 2], test$trial)
perf = performance(predROCR, 'tpr', 'fpr')
plot(perf, colorize = TRUE)
performance(predROCR, 'auc')@y.values
setwd('/Users/admin/Documents/Coursera/AnalyticsEdge-edX/Week5/Assignment')
emails = read.csv('emails.csv', stringsAsFactors = FALSE)
str(emails)
table(emails$spam)
email$text[[1]]
emails$text[[1]]
emails$text[[2]]
max(nchar(emails$text))
which.min(nchar(email$text))
which.min(nchar(emails$text))
library(tm)
emailCorpus = Corpus(VectorSource(emails$text))
emailCorpus = tm_map (emailCorpus, tolower)
emailCorpus = tm_map (emailCorpus, PlainTextDocument)
emailCorpys = tm_map (emailCorpus, removePunctuation)
emailCorpus = tm_map (emailCorpus, removePunctuation)
emailCorpus = tm_map (emailCorpus, removeWords, stopwords('English'))
emailCorpus = tm_ map (emailCorpus, stemDocument)
library(tm)
emailCorpus = Corpus(VectorSource(emails$text))
emailCorpus = tm_map (emailCorpus, tolower)
emailCorpus = tm_map (emailCorpus, PlainTextDocument)
emailCorpus = tm_map (emailCorpus, removePunctuation)
emailCorpus = tm_map (emailCorpus, removeWords, stopwords('English'))
emailCorpus = tm_map (emailCorpus, stemDocument)
dtm = DocumentTermMatrix(emailCorpus)
dtm
spdtm = removeSparseTerms(dtm, 0.95)
spdtm
emailSparse = as.data.frame(as.matrix(spdtm))
?make.names
colnames(emailSparse) = make.names(colnames(emailSparse))
which.max(colSums(emailSparse))
str(emails)
emailSparse$spam = emails$spam
names(emailSparse)
table(emailSparse$spam)
table(emailSparse[, - emailSparse$spam], emailSparse$spam, sum)
tapply(emailSparse[, - emailSparse$spam], emailSparse$spam, sum)
tapply(emailSparse[, - emailSparse$spam], emailSparse$spam, FUN = colSums)
head(emailSparse[, - emailSparse$spam])
by(emailSparse[, - emailSparse$spam], emailSparse$spam, FUN = colSums)
sort(by(emailSparse[, - emailSparse$spam], emailSparse$spam, FUN = colSums))
by(emailSparse[, - emailSparse$spam], emailSparse$spam, FUN = colSums)
class(by(emailSparse[, - emailSparse$spam], emailSparse$spam, FUN = colSums))
aggregate(emailSparse[, - emailSparse$spam], by = list(emailSparse$spam), FUN = sum)
aggregate(emailSparse[, - emailSparse$spam], by = list(emailSparse$spam), FUN = sum)[, 'Group.1' == 0]
class(aggregate(emailSparse[, - emailSparse$spam], by = list(emailSparse$spam), FUN = sum))
wordAgg = aggregate(emailSparse[, - emailSparse$spam], by = list(emailSparse$spam), FUN = sum)
subset(wordAgg, wordAgg$Group.0 == 0)
wordAgg[, wordAgg$Group.1 == 0]
wordAgg[wordAgg$Group.1 == 0, ]
sort(wordAgg[wordAgg$Group.1 == 0, ])
sort(wordAgg[wordAgg$Group.1 == 1, ])
emailSparse$spam = as.factor(emailSparse$spam)
library(caTools)
set.seed(123)
spl = sample.split(emailSparse$spam, SplitRatio = 0.7)
train = subset(emailSparse, spl == TRUE)
test = subset(emailSparse, spl == FALSE)
logModel = glm(spam ~ ., data = train, family = binomial)
spamLog = glm(spam ~ ., data = train, family = binomial)
spamCART = rpart(spam ~ ., data = train, method = 'class')
spamRF = randomForest(spam ~ ., data = train)
logPred = predict(spamLog)
cartPred = predict(spamCART)
rfPred = predict(spamRF, type = 'prob')
head(logPred)
head(cartPred)
head(rfPred)
which(cartPred > 0.9999)
length(cartPred > 0.9999)
length(logPred < 0.00001)
lrngth(rfPred < 0.00001 & rfPred > 0.99999)
length(rfPred < 0.00001 & rfPred > 0.99999)
length(rfPred > 0.99999)
length(rfPred < 0.00001)
nrow(rfPred)
nrow(cartPred)
which(rfPred < 0.00001 & rfPred > 0.9999)
which(rfPred < 0.00001 || rfPred > 0.9999)
which(rfPred < 0.00001 or rfPred > 0.9999)
which(rfPred < 0.00001 | rfPred > 0.9999)
length(which(logPred < 0.00001))
length(which(cartPred > 0.9999))
length(which(rfPred < 0.00001 | rfPred > 0.9999))
length(which(logPred > 0.9999))
length(which(logPred < 0.00001 | rfPred > 0.9999))
length(which(logPred < 0.00001 | logPred > 0.9999))
length(which((logPred < 0.00001) & (logPred > 0.9999)))
length(which((logPred > 0.00001) & (logPred < 0.9999)))
length(which(logPred > 0.00001 & logPred < 0.9999))
summary(spamLog)
prp(spamCART)
table(train$spam, carPred[, 2] > 0.5)
table(train$spam, cartPred[, 2] > 0.5)
cartAccuracy = (2885 + 894) / nrow(train)
cartAccuracy
library(ROCR)
?prediction
pred = prediction(logPred, train$spam)
perf = performance(pred, 'tpr', 'fpr')
performance(pred, 'auc')@y.values
table(train$spam, logPred > 0.5)
logAccuracy
logAccuracy = (3052 + 954) / nrow(train)
logAccuracy
table(train$spam, cartPred[, 2] > 0.5)
cartAccuracy = (2885 + 894) / nrow(train)
cartAccuracy
cartPrediction = prediction(cartPred, train$spam)
cartPrediction = prediction(cartPred[,2], train$spam)
performance(cartPrediction, 'auc')@y.values
table(train$spam, rfPred[, 2] > 0.5)
rfAccuracy = (3016 + 918) / nrow(train)
rfAccuracy
rfPrediction = prediction(rfPred, train$spam)
performance(rfPrediction, 'auc')@y.values
rfPrediction = prediction(rfPred[, 2], train$spam)
performance(rfPrediction, 'auc')@y.values
logPred = predict(spamLog, newdata = test)
cartPred = predict(spamCART, newdata = test)
rfPred = predict(spamRF, newdata = test)
table(test$spam, logPred > 0.5)
logAccuracy = (1258 + 376) / nrow(test)
logAccuracy
logPrediction = prediction(logPred, test$spam)
performance(logPrediction, 'auc')@y.values
table(test$spam, cartPred[, 2] > 0.5)
cartAccuracy = (1228 + 386) / nrow(test)
cartAccuracy
cartPrediction = prediction(cartPred[, 2], test$spam)
performance(cartPrediction, 'auc')@y.values
table(test$spam, rfPred[, 2] > 0.5)
head(rfPred)
rfPred = predict(spamRF, newdata = test)
head(rfPred)
head(predict(spamRF))
rfPred = predict(spamRF, newdata = test, type = 'prob')
table(test$spam, rfPred[, 2] > 0.5)
rfAccuracy = (1292 + 389) / nrow(test)
rfAccuracy
rfPrediction = prediction(rfPred[, 2], test$spam)
performance(rfPrediction, 'auc')@y.values
